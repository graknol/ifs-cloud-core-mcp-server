#!/usr/bin/env python3
"""
Check Optimum TensorRT Capabilities
"""


def check_optimum_tensorrt():
    """Check what TensorRT capabilities are available via Optimum."""

    print("üîç Checking Optimum TensorRT Capabilities")
    print("=" * 45)

    # Check optimum installation
    try:
        import optimum

        print("‚úÖ Optimum installed")
    except ImportError:
        print("‚ùå Optimum not installed")
        return

    # Check optimum.nvidia
    try:
        from optimum.nvidia import TensorRTForSequenceClassification

        print("‚úÖ optimum.nvidia.TensorRTForSequenceClassification available")
        tensorrt_via_nvidia = True
    except ImportError as e:
        print(f"‚ùå optimum.nvidia not available: {e}")
        tensorrt_via_nvidia = False

    # Check optimum.onnxruntime (which we have)
    try:
        from optimum.onnxruntime import ORTModelForSequenceClassification

        print("‚úÖ optimum.onnxruntime.ORTModelForSequenceClassification available")
        ort_available = True
    except ImportError as e:
        print(f"‚ùå optimum.onnxruntime not available: {e}")
        ort_available = False

    # Check direct tensorrt import
    try:
        import tensorrt as trt

        print(f"‚úÖ Direct TensorRT import available: version {trt.__version__}")
        direct_tensorrt = True
    except ImportError as e:
        print(f"‚ùå Direct TensorRT not available: {e}")
        direct_tensorrt = False

    # Check for TensorRT execution providers in ONNX Runtime
    if ort_available:
        try:
            import onnxruntime as ort

            providers = ort.get_available_providers()
            print(f"üìä ONNX Runtime providers: {providers}")
            if "TensorrtExecutionProvider" in providers:
                print("‚úÖ TensorRT execution provider available in ONNX Runtime")
            else:
                print("‚ùå TensorRT execution provider not available in ONNX Runtime")
        except Exception as e:
            print(f"‚ùå Error checking ONNX Runtime providers: {e}")

    print(f"\nüìã SUMMARY:")
    print(f"   Optimum NVIDIA: {'‚úÖ' if tensorrt_via_nvidia else '‚ùå'}")
    print(f"   Optimum ONNX Runtime: {'‚úÖ' if ort_available else '‚ùå'}")
    print(f"   Direct TensorRT: {'‚úÖ' if direct_tensorrt else '‚ùå'}")

    if ort_available and not tensorrt_via_nvidia:
        print(f"\nüí° RECOMMENDATION:")
        print(
            f"   You have optimum[onnxruntime-gpu] which can provide GPU acceleration"
        )
        print(f"   through CUDA/DirectML execution providers, even without TensorRT.")
        print(f"   This is often sufficient for good performance on RTX 5070 Ti.")

    if not any([tensorrt_via_nvidia, direct_tensorrt]):
        print(f"\nüîß TO GET TENSORRT:")
        print(f"   1. For Linux: pip install optimum[nvidia]")
        print(f"   2. For Windows: Download TensorRT from NVIDIA Developer site")
        print(f"   3. Alternative: Use current setup with torch.compile + max-autotune")


if __name__ == "__main__":
    check_optimum_tensorrt()
